---
title: "publication"
bg: blue
color: white
fa-icon: quote-left
---

*We introduce PathGAN, a deep neural network for visual scanpath prediction trained on adversarial examples. A visual scanpath is defined as the sequence of fixation points over an image defined by a human observer with its gaze. PathGAN is composed of two parts, the generator and the discriminator. Both parts extract features from images using off-the-shelf networks, and train recurrent layers to generate or discriminate scanpaths accordingly. In scanpath prediction, the stochastic nature of the data makes it very difficult to generate realistic predictions using supervised learning strategies, but we adopt adversarial training as a suitable alternative. Our experiments prove how PathGAN improves the state of the art of visual scanpath prediction on the iSUN and Salient360! datasets.*

Find the full paper on [arXiv](https://arxiv.org/abs/1809.00567) or download the PDF directly from [here](https://github.com/imatge-upc/pathgan/raw/gh-pages/assens-2018-pathgan.pdf).

If you find this work useful, please consider citing:

<i>
Marc Assens, Xavier Giro-i-Nieto, Kevin McGuinness, Noel E. O'Connor. "PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks", ECCV Workshop on Egocentric Perception, Interaction and Computing (EPIC), 2018.
</i>

<pre>
@inproceedings{Assens2018pathgan,
title={PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks},
author={Marc Assens, Xavier Giro-i-Nieto, Kevin McGuinness, Noel E. O'Connor},
journal={ECCV Workshop on Egocentric Perception, Interaction and Computing (EPIC)},
year={2018}
}
</pre>

<!-- If you find this work useful, please consider citing:

```
Amaia Salvador, Miriam Bellver, Manel Baradad, Ferran Marques, Jordi Torres, Xavier Giro-i-Nieto, "Recurrent Neural Networks for Semantic Instance Segmentation" arXiv:1712.00617 (2017).
```
 -->
<!-- Download our paper in pdf [here](https://github.com/imatge-upc/rsis/raw/gh-pages/assets/rsis.pdf) or on [arXiv](https://arxiv.org/abs/1712.00617).  -->
